{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7b87630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510b201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc57077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf1e1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNet(nn.Module):\n",
    "    # Neural Network for Alpha Prediction\n",
    "    def __init__(self, input_dim, hidden_dims = [128, 64, 32],\n",
    "                 dropout_rate = 0.3, use_batch_norm = True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear Layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "\n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            # Drop out\n",
    "            if i < len(hidden_dims) -1:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45af9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaTrainer:\n",
    "    def __init__(self, model_config = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Default\n",
    "        self.config = {\n",
    "            'hidden_dims': [128, 64, 32],\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 512,\n",
    "            'epochs': 100,\n",
    "            'patience': 15,\n",
    "            'weight_decay': 1e-5,\n",
    "            'scheduler_patience': 7,\n",
    "            'scheduler_factor': 0.5\n",
    "        }\n",
    "        if model_config is not None:\n",
    "            self.config.update(model_config)\n",
    "        \n",
    "        # Feature names\n",
    "        self.feature_names = [\n",
    "            'daily_return', 'FEAT_DebtEquity_quarterly', 'FEAT_PE_quarterly',\n",
    "            'FEAT_EVEBITDA_quarterly', 'FEAT_ROE_quarterly', 'price_mom_5d',\n",
    "            'price_mom_10d', 'price_mom_20d', 'vol_w_mom_5d', 'vol_w_mom_10d',\n",
    "            'vol_w_mom_20d', 'volatility_10d', 'skewness_20d', 'rsi_14d',\n",
    "            'zscore_mom_10d_60w'\n",
    "        ]\n",
    "\n",
    "        # Scalers\n",
    "        self.feature_scaler = RobustScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "\n",
    "        # Training history\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'ic': [], 'rank_ic': []} \n",
    "    \n",
    "    def prepare_data(self, stock_df_path, market_df_path, test_size = 0.2):\n",
    "        features_cols = self.feature_names\n",
    "        dfs = []\n",
    "        for path in stock_df_path:\n",
    "            p = Path(path).resolve()\n",
    "            exchange = p.parts[-2]\n",
    "            df = pd.read_csv(p)\n",
    "            df['exchange'] = exchange\n",
    "            dfs.append(df)\n",
    "        df_stock_all = pd.concat(dfs, ignore_index= True)\n",
    "        dfs = []\n",
    "        for path in market_df_path:\n",
    "            p = Path(path).resolve()\n",
    "            name = p.stem.upper()\n",
    "            exchange = \"HNX\" if \"HNX\" in name else \"HOSE\"\n",
    "            df = pd.read_csv(p)\n",
    "            df['exchange'] = exchange\n",
    "            dfs.append(df)\n",
    "        df_market_all = pd.concat(dfs, ignore_index= True)\n",
    "        df_market_subset = df_market_all[['time', 'exchange', 'FUT_RET_10D_market']]        \n",
    "        merged_df = pd.merge(df_stock_all, df_market_subset, on = ['time', 'exchange'], how = 'inner' )\n",
    "        merged_df['alpha'] = merged_df['FUT_RET_10D'] - merged_df['FUT_RET_10D_market']\n",
    "        merged_df = merged_df.dropna() \n",
    "        #Separate features and target\n",
    "        X = merged_df[features_cols].values\n",
    "        y = merged_df['alpha'].values\n",
    "\n",
    "        #Train test split\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        #Scale features and target\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.feature_scaler.transform(X_test)\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test_scaled = self.target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "        print(f\"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}\")\n",
    "\n",
    "        return (X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled,\n",
    "                y_train, y_test, features_cols)\n",
    "    \n",
    "    def calculate_ic(self, predictions, targets):\n",
    "        # Remove NaN values\n",
    "        valid_mask = ~(np.isnan(predictions) | np.isnan(targets))\n",
    "        pred_clean = predictions[valid_mask]\n",
    "        target_clean = targets[valid_mask]\n",
    "        \n",
    "        if len(pred_clean) < 10:  # Need minimum samples\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Pearson correlation (IC)\n",
    "        ic = np.corrcoef(pred_clean, target_clean)[0, 1]\n",
    "        if np.isnan(ic):\n",
    "            ic = 0.0\n",
    "        \n",
    "        # Spearman rank correlation (Rank IC)\n",
    "        from scipy.stats import spearmanr\n",
    "        rank_ic, _ = spearmanr(pred_clean, target_clean)\n",
    "        if np.isnan(rank_ic):\n",
    "            rank_ic = 0.0\n",
    "        \n",
    "        return ic, rank_ic\n",
    "    \n",
    "    def train_model(self, X_train, X_val, y_train, y_val, y_val_unscaled):\n",
    "        # Train the neural network\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = AlphaDataset(X_train, y_train)\n",
    "        val_dataset = AlphaDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size= self.config['batch_size'], shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = self.config['batch_size'], shuffle= False)\n",
    "\n",
    "        # Initialize mode\n",
    "        model = AlphaNet(\n",
    "            input_dim = X_train.shape[1],\n",
    "            hidden_dims = self.config['hidden_dims'],\n",
    "            dropout_rate = self.config['dropout_rate'],\n",
    "            use_batch_norm = self.config['use_batch_norm']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr = self.config['learning_rate'],\n",
    "                               weight_decay = self.config['weight_decay'])\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode = 'min', patience = self.config['scheduler_patience'],\n",
    "            factor = self.config['scheduler_factor']\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = 0\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm= 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    val_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            # Unscale predictions for IC calculation\n",
    "            val_pred_unscaled = self.target_scaler.inverse_transform(\n",
    "                np.array(val_predictions).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            \n",
    "            ic, rank_ic = self.calculate_ic(val_pred_unscaled, y_val_unscaled)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['ic'].append(ic)\n",
    "            self.history['rank_ic'].append(rank_ic)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{self.config[\"epochs\"]}], '\n",
    "                      f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, '\n",
    "                      f'IC: {ic:.4f}, Rank IC: {rank_ic:.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= self.config['patience']:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        return model      \n",
    "\n",
    "    def evaluate_model(self, model, X_test,\n",
    "                       y_test_scaled, y_test_unscaled):\n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            X_test_tensor = torch.FloatTensor(X_test).to(self.device)\n",
    "            predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "            predictions_scaled_reshaped = predictions_scaled.reshape(-1, 1)\n",
    "\n",
    "            predictions_unscaled = self.target_scaler.inverse_transform(\n",
    "                predictions_scaled_reshaped\n",
    "            ).flatten()\n",
    "            # Unscaled loss\n",
    "            mse = mean_squared_error(y_test_unscaled, predictions_unscaled)\n",
    "            mae = mean_absolute_error(y_test_unscaled, predictions_unscaled)\n",
    "            ic, rank_ic = self.calculate_ic(predictions_unscaled, y_test_unscaled)\n",
    "\n",
    "            print(f\"\\nEvaluation on Test set:\")\n",
    "            print(f\"MSE: {mse:.6f}\")\n",
    "            print(f\"MAE: {mae:.6f}\")\n",
    "            print(f\"IC: {ic:.4f}\")\n",
    "            print(f\"Rank IC: {rank_ic:.4f}\")\n",
    "\n",
    "        return predictions_unscaled, {'mse': mse, 'mae': mae, 'ic': ic, 'rank_ic': rank_ic}          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01b31f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_alpha_model(stock_df_path, market_df_path, config = None):\n",
    "\n",
    "    trainer = AlphaTrainer()\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train_scaled, y_test_scaled, y_train, y_test, features_cols = trainer.prepare_data(stock_df_path, market_df_path)\n",
    "\n",
    "    # Train model\n",
    "    model = trainer.train_model(X_train, X_test, y_train_scaled, y_test_scaled, y_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    predictions, metrics = trainer.evaluate_model(model, X_test, y_test_scaled, y_test)\n",
    "    \n",
    "    return model, trainer, predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "030b9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "current_script_directory = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "input_base_folder = os.path.join(current_script_directory, '..', 'data_prep')\n",
    "stock_features_path = os.path.join(input_base_folder, 'calculated_stock_features')\n",
    "market_indices_path = os.path.join(input_base_folder, 'stock_data\\market_indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff32f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_path = glob.glob(os.path.join(stock_features_path, \"*\", \"*.csv\"))\n",
    "market_df_path = glob.glob(os.path.join(market_indices_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92886632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (664739, 15), Test set: (166185, 15)\n",
      "Epoch [10/100], Train Loss: 0.988329, Val Loss: 0.679555, IC: 0.0729, Rank IC: 0.0697\n",
      "Epoch [20/100], Train Loss: 0.985254, Val Loss: 0.678091, IC: 0.0875, Rank IC: 0.0811\n",
      "Epoch [30/100], Train Loss: 0.983203, Val Loss: 0.678065, IC: 0.0891, Rank IC: 0.0837\n",
      "Epoch [40/100], Train Loss: 0.981161, Val Loss: 0.677731, IC: 0.0895, Rank IC: 0.0880\n",
      "Epoch [50/100], Train Loss: 0.979822, Val Loss: 0.678361, IC: 0.0854, Rank IC: 0.0857\n",
      "Epoch [60/100], Train Loss: 0.978949, Val Loss: 0.677962, IC: 0.0894, Rank IC: 0.0874\n",
      "Epoch [70/100], Train Loss: 0.978627, Val Loss: 0.678225, IC: 0.0896, Rank IC: 0.0868\n",
      "Epoch [80/100], Train Loss: 0.977818, Val Loss: 0.678385, IC: 0.0859, Rank IC: 0.0845\n",
      "Early stopping at epoch 83\n",
      "\n",
      "Evaluation on Test set:\n",
      "MSE: 65.867974\n",
      "MAE: 5.424329\n",
      "IC: 0.0916\n",
      "Rank IC: 0.0888\n"
     ]
    }
   ],
   "source": [
    "trainer = AlphaTrainer()\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train_scaled, y_test_scaled, y_train, y_test, features_cols = trainer.prepare_data(stock_df_path, market_df_path)\n",
    "\n",
    "# Train model\n",
    "model = trainer.train_model(X_train, X_test, y_train_scaled, y_test_scaled, y_test)\n",
    "\n",
    "# Evaluate model\n",
    "predictions, metrics = trainer.evaluate_model(model, X_test, y_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d36d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha_signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
