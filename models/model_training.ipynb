{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3aed5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510b201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fc57077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNet(nn.Module):\n",
    "    # Neural Network for Alpha Prediction\n",
    "    def __init__(self, input_dim, hidden_dims = [128, 64, 32],\n",
    "                 dropout_rate = 0.3, use_batch_norm = True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear Layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "\n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            # Drop out\n",
    "            if i < len(hidden_dim) -1:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaTrainer:\n",
    "    def __init__(self, model_config):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Default\n",
    "        self.config = {\n",
    "            'hidden_dims': [128, 64, 32],\n",
    "            'dropout_rate': 0.3,\n",
    "            'use_batch_norm': True,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 512,\n",
    "            'epochs': 100,\n",
    "            'patience': 15,\n",
    "            'weight_decay': 1e-5,\n",
    "            'scheduler_patience': 7,\n",
    "            'scheduler_factor': 0.5\n",
    "        }\n",
    "        if model_config:\n",
    "            self.config.update(model_config)\n",
    "        \n",
    "        # Feature names\n",
    "        self.feature_names = [\n",
    "            'daily_return', 'FEAT_DebtEquity_quarterly', 'FEAT_PE_quarterly',\n",
    "            'FEAT_EVEBITDA_quarterly', 'FEAT_ROE_quarterly', 'price_mom_5d',\n",
    "            'price_mom_10d', 'price_mom_20d', 'vol_w_mom_5d', 'vol_w_mom_10d',\n",
    "            'vol_w_mom_20d', 'volatility_10d', 'skewness_20d', 'rsi_14d',\n",
    "            'zscore_mom_10d_60w'\n",
    "        ]\n",
    "\n",
    "        # Scalers\n",
    "        self.feature_scaler = RobustScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "\n",
    "        # Training history\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'ic': [], 'rank_ic': []} \n",
    "    \n",
    "    def prepare_data(self, stock_df_path, market_df_path, test_size = 0.2):\n",
    "        features_cols = self.feature_names\n",
    "        dfs = []\n",
    "        for path in stock_df_path:\n",
    "            p = Path(path).resolve()\n",
    "            exchange = p.parts[-2]\n",
    "            df = pd.read_csv(p)\n",
    "            df['exchange'] = exchange\n",
    "            dfs.append(df)\n",
    "        df_stock_all = pd.concat(dfs, ignore_index= True)\n",
    "        dfs = []\n",
    "        for path in market_df_path:\n",
    "            p = Path(path).resolve()\n",
    "            name = p.stem.upper()\n",
    "            exchange = \"HNX\" if \"HNX\" in name else \"HOSE\"\n",
    "            df = pd.read_csv(p)\n",
    "            df['exchange'] = exchange\n",
    "            dfs.append(df)\n",
    "        df_market_all = pd.concat(dfs, ignore_index= True)\n",
    "        df_market_subset = df_market_all[['time', 'exchange', 'FUT_RET_10D_market']]        \n",
    "        merged_df = pd.merge(df_stock_all, df_market_subset, on = ['time', 'exchange'], how = 'inner' )\n",
    "        merged_df['alpha'] = merged_df['FUT_RET_10D'] - merged_df['FUT_RET_10D_market']\n",
    "        \n",
    "        #Separate features and target\n",
    "        X = merged_df[features_cols].values\n",
    "        y = merged_df['alpha'].values\n",
    "\n",
    "        #Train test split\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        #Scale features and target\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.feature_scaler.transform(X_test)\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test_scaled = self.target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "        print(f\"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}\")\n",
    "\n",
    "        return (X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled,\n",
    "                y_train, y_test, features_cols)\n",
    "    \n",
    "    def calculate_ic(self, predictions, targets):\n",
    "        # Remove NaN values\n",
    "        valid_mask = ~(np.isnan(predictions) | np.isnan(targets))\n",
    "        pred_clean = predictions[valid_mask]\n",
    "        target_clean = targets[valid_mask]\n",
    "        \n",
    "        if len(pred_clean) < 10:  # Need minimum samples\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Pearson correlation (IC)\n",
    "        ic = np.corrcoef(pred_clean, target_clean)[0, 1]\n",
    "        if np.isnan(ic):\n",
    "            ic = 0.0\n",
    "        \n",
    "        # Spearman rank correlation (Rank IC)\n",
    "        from scipy.stats import spearmanr\n",
    "        rank_ic, _ = spearmanr(pred_clean, target_clean)\n",
    "        if np.isnan(rank_ic):\n",
    "            rank_ic = 0.0\n",
    "        \n",
    "        return ic, rank_ic\n",
    "    \n",
    "    def train_model(self, X_train, X_val, y_train, y_val, y_val_unscaled):\n",
    "        # Train the neural network\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = AlphaDataset(X_train, y_train)\n",
    "        val_dataset = AlphaDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size= self.config['batch_size'], shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = self.config['batch_size'], shuffle= False)\n",
    "\n",
    "        # Initialize mode\n",
    "        model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b31f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_alpha_model(stock_df_path, market_df_path, config):\n",
    "\n",
    "    trainer = AlphaTrainer(config)\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train_scaled, y_test_scaled, y_train, y_test, features_cols = trainer.prepare_data(stock_df_path, market_df_path)\n",
    "\n",
    "    # Train model\n",
    "    #model = trainer.train_model(X_train, X_test, y_train_scaled, y_test_scaled, y_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "030b9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "current_script_directory = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "input_base_folder = os.path.join(current_script_directory, '..', 'data_prep')\n",
    "stock_features_path = os.path.join(input_base_folder, 'calculated_stock_features')\n",
    "market_indices_path = os.path.join(input_base_folder, 'stock_data\\market_indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff32f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_path = glob.glob(os.path.join(stock_features_path, \"*\", \"*.csv\"))\n",
    "market_df_path = glob.glob(os.path.join(market_indices_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b680aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\ADMIN\\\\PROJECT\\\\alpha_signal_predictor\\\\models\\\\..\\\\data_prep\\\\stock_data\\\\market_indices\\\\HNXINDEX.csv', 'c:\\\\Users\\\\ADMIN\\\\PROJECT\\\\alpha_signal_predictor\\\\models\\\\..\\\\data_prep\\\\stock_data\\\\market_indices\\\\VNINDEX.csv']\n"
     ]
    }
   ],
   "source": [
    "print(market_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe2f30d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (666792, 15), Test set: (166698, 15)\n",
      "Training set: (666792,), Test set: (166698,)\n"
     ]
    }
   ],
   "source": [
    "train_alpha_model(stock_df_path, market_df_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha_signal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
